{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3L2q1MN4SOUU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import pickle\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import collections\n",
        "import scipy.misc\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.transform import resize\n",
        "import imageio\n",
        "from collections import Counter\n",
        "\n",
        "# possible neural network architectures\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.mobilenet import MobileNet\n",
        "\n",
        "# and corresponding pre-processing\n",
        "from keras.applications.imagenet_utils import preprocess_input\n",
        "\n",
        "# Keras layers and utils\n",
        "import keras.backend as K\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input, Dropout, Flatten, Dense, BatchNormalization, AveragePooling2D\n",
        "from keras.layers import Conv2D, GlobalAveragePooling2D, MaxPooling2D, UpSampling2D\n",
        "from keras.layers import Activation, Reshape\n",
        "from keras import optimizers\n",
        "from keras.regularizers import l2\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, Callback\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "\n",
        "\n",
        "# transformations\n",
        "datagen =ImageDataGenerator(rotation_range=40,\n",
        "                               width_shift_range=0.1,\n",
        "                               height_shift_range=0.1,\n",
        "                               shear_range=0.01,\n",
        "                               zoom_range=[0.9, 1.25],\n",
        "                               horizontal_flip=True,\n",
        "                               vertical_flip=False,\n",
        "                               fill_mode='reflect',\n",
        "                               data_format='channels_last',\n",
        "                           )\n",
        "\n",
        "def get_filenames_and_classes(csv_path):\n",
        "    df = pd.read_csv(csv_path, sep='\\t', index_col=0)\n",
        "    path_classes, train_classes = np.array([]), np.array([])\n",
        "    for path, classes in zip(df.index, df.iloc[:,0]):\n",
        "        train_classes = np.append(train_classes,int(classes))\n",
        "        path_classes = np.append(path_classes,str(path))\n",
        "    return path_classes, train_classes\n",
        "\n",
        "def open_jpg(img):\n",
        "    return imageio.imread(img)\n",
        "\n",
        "# Create balanced generators\n",
        "def get_batches(path_classes,train_classes):\n",
        "    while True:\n",
        "        batch_input = np.zeros((32,224,224,3))\n",
        "        batch_output = np.zeros((32,2))\n",
        "        cond = 0\n",
        "        for i in range(32):\n",
        "            temp = 0\n",
        "            index = np.random.permutation(len(train_classes))\n",
        "            while temp == 0:\n",
        "                if (int(train_classes[(index[i])]) == 1) & (cond%2==0):\n",
        "                    temp = 1\n",
        "                    cond = cond + 1\n",
        "                    img = open_jpg(path_classes[(index[i])])\n",
        "                    img = img.reshape((1,) + img.shape)\n",
        "                    img = datagen.flow(img, batch_size=1).next().reshape(750,750,3)\n",
        "                    imgr = resize(img, (224, 224), anti_aliasing=True)\n",
        "                    batch_input[i] = imgr\n",
        "                    batch_output[i][0] = 1\n",
        "                elif (int(train_classes[(index[i])]) == 2) & (cond%2==1):\n",
        "                    temp = 1\n",
        "                    cond = cond + 1\n",
        "                    img = open_jpg(path_classes[(index[i])])\n",
        "                    img = img.reshape((1,) + img.shape)\n",
        "                    img = datagen.flow(img, batch_size=1).next().reshape(750,750,3)\n",
        "                    imgr = resize(img, (224, 224), anti_aliasing=True)\n",
        "                    batch_input[i] = imgr\n",
        "                    batch_output[i][1] = 1\n",
        "                else:\n",
        "                    index = np.random.permutation(len(train_classes))\n",
        "        yield batch_input, batch_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfoJ3XboSYfb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "    #--------train batches\n",
        "    path_classes, train_classes = np.array([]), np.array([])\n",
        "    for pref in ['ZT199', 'ZT204', 'ZT111']:\n",
        "        #change this for your path\n",
        "        path_csv = \"/home/fabianleon/Documentos/gleason_CNN-master/dataset/tma_info/\" + str(pref) + \"_pgleason_scores.csv\"\n",
        "        print(path_csv)\n",
        "        path, classes = get_filenames_and_classes(path_csv)\n",
        "        path_classes = np.append(path_classes,path)\n",
        "        train_classes = np.append(train_classes,classes)\n",
        "    train_batches = get_batches(path_classes,train_classes)\n",
        "    print(collections.Counter(train_classes))\n",
        "    #--------validation batches\n",
        "    pathv_classes, validation_classes = np.array([]), np.array([])\n",
        "    #change this for your path\n",
        "    path_csv = \"/home/fabianleon/Documentos/gleason_CNN-master/dataset/tma_info/ZT76_pgleason_scores.csv\"\n",
        "    print(path_csv)\n",
        "    pathv_classes, validation_classes = get_filenames_and_classes(path_csv)\n",
        "    validation_batches = get_batches(pathv_classes,validation_classes)\n",
        "    print(collections.Counter(validation_classes)) \n",
        "    n_class = 2\n",
        "    #change this for your path\n",
        "    mpath = '/home/fabianleon/Documentos/gleason_CNN-master/model_weights/class1y2incep'\n",
        "    # create model\n",
        "    if not os.path.exists(mpath):\n",
        "        os.makedirs(mpath)\n",
        "    model_weights = os.path.join(mpath, 'model_{epoch:02d}.h5')\n",
        "    base_model = InceptionV3(include_top=False, weights='imagenet', input_shape=(224, 224, 3), pooling='avg')\n",
        "    x_top = base_model.output\n",
        "    x_out = Dense(n_class, name='output', activation='softmax')(x_top)\n",
        "    model = Model(base_model.input, x_out)\n",
        "    model.summary()\n",
        "    # train only softmax\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False                   \n",
        "    model.compile(optimizer=optimizers.Adam(lr=0.001),\n",
        "                  loss='categorical_crossentropy', metrics = ['accuracy'])\n",
        "    history = model.fit_generator(\n",
        "                generator=train_batches,\n",
        "\t\t            use_multiprocessing=True,\n",
        "                steps_per_epoch=200,\n",
        "                epochs=5,\n",
        "                validation_data=validation_batches,\n",
        "                validation_steps=100,\n",
        "                verbose=1\n",
        "    )\n",
        "    # train all layers\n",
        "    for layer in model.layers:\n",
        "        layer.trainable = True\n",
        "\n",
        "    model.compile(optimizer=optimizers.SGD(lr=0.0001, momentum=0.9),\n",
        "                  loss='categorical_crossentropy', metrics = ['accuracy'])\n",
        "    checkpoint = ModelCheckpoint(filepath=model_weights, save_best_only=False, verbose=0)\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=10, min_lr=0.00001)\n",
        "\n",
        "    history = model.fit_generator(\n",
        "                generator=train_batches,\n",
        "\t\t            use_multiprocessing=True,\n",
        "                steps_per_epoch=200,\n",
        "                epochs=50,\n",
        "                callbacks=[checkpoint, reduce_lr],\n",
        "                validation_data=validation_batches,\n",
        "                validation_steps=100,\n",
        "                verbose=1\n",
        "    )\n",
        "    with open(os.path.join(mpath, 'history.pkl'), 'wb') as history_f:\n",
        "        pickle.dump(history.history, history_f, protocol=2)\n",
        "    \n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}